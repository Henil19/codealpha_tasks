# -*- coding: utf-8 -*-
"""Music_Generation_With_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QhxkwDNy5PWOW0gxjh4nCz_zLMDyyVe2

# Music Generation with AI — Colab Notebook
**Goal:** collect MIDI files, preprocess into note sequences (using `music21`), train an LSTM-based model, generate new sequences, and convert them back to MIDI/audio.

**Features included**
- Upload or mount Google Drive MIDI dataset.
- `music21`-based parsing + quantization to time-step events (polyphony supported).
- Tokenization (time-step events → integer tokens) and saved token maps.
- LSTM model (Embedding + 2×LSTM) with checkpoints and early stopping.
- Training logs + loss/accuracy plots.
- Sampling with temperature and seed control.
- Convert generated sequences → MIDI and render to WAV/MP3 (timidity + ffmpeg).
- Reproducibility: random seeds, saved `best_music_model.h5`, `token_to_int.json`, `int_to_token.json`.

**How to use**
1. Upload `.mid` / `.midi` files into `/content/midi_data` (or mount Drive and set `DATA_ROOT`).
2. Run cells in order. Use the generation cell to tweak `temperature`, `gen_length`, and `seed_choice`.
3. Listen to generated outputs inline or download the MIDI/MP3 files.

# Step 1: Install dependencies
"""

!pip install pretty_midi music21 tensorflow==2.19.0 tqdm

"""# Step 2: Imports, seeds, and config"""

import os, glob, json, random, math
from tqdm import tqdm
import numpy as np
import pretty_midi
import music21
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

# reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)
os.environ['PYTHONHASHSEED'] = str(SEED)

print("TensorFlow:", tf.__version__)
print("music21:", music21.__version__)

"""# Step 3: Mount Google Drive"""

from google.colab import drive
drive.mount('/content/drive')
DATA_ROOT = "/content/drive/MyDrive/midi_dataset"

"""# Step 4: list MIDI files & quick stats"""

midi_files = glob.glob(os.path.join(DATA_ROOT, "*.mid")) + glob.glob(os.path.join(DATA_ROOT, "*.midi"))
print("Found MIDI files:", len(midi_files))
for i, f in enumerate(midi_files[:10]):
    print(i+1, os.path.basename(f))

"""# Step 5: Music21-based MIDI → time-step sequence function"""

from music21 import converter, note, chord, tempo

# Tunable parameters
TIME_STEP = 0.25  # in quarterLength units (0.25 = 16th note if quarter=1)
MIN_NOTE = 21     # A0
MAX_NOTE = 108    # C8 (piano range)
MAX_PIECES = None # set to int to limit number of pieces parsed (for quick runs)

def midi_to_sequence_music21(path, time_step=TIME_STEP, min_note=MIN_NOTE, max_note=MAX_NOTE):
    """
    Parse a MIDI file with music21, extract notes/chords, quantize to time steps (quarterLength units).
    Returns list of tuples: each time-step is a sorted tuple of active MIDI pitch ints.
    """
    try:
        s = converter.parse(path)
    except Exception as e:
        print("music21 parse error for", os.path.basename(path), ":", e)
        return None

    # Try to get tempo if present (we keep everything in quarterLength units; tempo not needed)
    flat = s.flat
    notes = []
    for el in flat:
        if isinstance(el, note.Note):
            notes.append((float(el.offset), [int(el.pitch.midi)], float(el.duration.quarterLength)))
        elif isinstance(el, chord.Chord):
            notes.append((float(el.offset), [int(p.midi) for p in el.pitches], float(el.duration.quarterLength)))

    if not notes:
        return []

    end_time = max(off + dur for off, _, dur in notes)
    n_steps = int(np.ceil(end_time / time_step))
    steps = [set() for _ in range(n_steps)]

    for off, pitches, dur in notes:
        start_idx = int(np.floor(off / time_step))
        end_idx = int(np.ceil((off + dur) / time_step))
        for i in range(max(0, start_idx), min(n_steps, end_idx)):
            for p in pitches:
                if min_note <= p <= max_note:
                    steps[i].add(p)

    seq = [tuple(sorted(s)) for s in steps]
    return seq

# Quick test (only if files exist)
if midi_files:
    seq = midi_to_sequence_music21(midi_files[0])
    print("Example sequence length:", len(seq))
    print("First 12 time-steps:", seq[:12])

"""# Step 6: Build corpus"""

def build_corpus(data_root, max_pieces=MAX_PIECES):
    files = glob.glob(os.path.join(data_root, "*.mid")) + glob.glob(os.path.join(data_root, "*.midi"))
    corpus = []
    for i, f in enumerate(tqdm(files, desc="Parsing MIDIs with music21")):
        if max_pieces and i >= max_pieces:
            break
        seq = midi_to_sequence_music21(f)
        if seq is not None and len(seq) > 4:
            corpus.append(seq)
    return corpus

corpus = build_corpus(DATA_ROOT)
print("Parsed pieces:", len(corpus))

"""# Step 7: Build vocabulary and save token maps"""

# Build vocabulary of unique time-step events
vocab = set()
for seq in corpus:
    for ev in seq:
        vocab.add(ev)
vocab = sorted(vocab, key=lambda x: (len(x), x))  # deterministic order
token_to_int = {tok: i for i, tok in enumerate(vocab)}
int_to_token = {i: tok for tok, i in token_to_int.items()}
VOCAB_SIZE = len(vocab)
print("Vocab size:", VOCAB_SIZE)

# Save maps
with open("token_to_int.json", "w") as f:
    # JSON can't have tuples as keys; convert tuple->string
    json.dump({str(k): int(v) for k, v in token_to_int.items()}, f)
with open("int_to_token.json", "w") as f:
    json.dump({str(k): list(v) for k, v in int_to_token.items()}, f)
print("Saved token maps: token_to_int.json, int_to_token.json")

"""# Step 8: Create training examples (sliding windows)"""

SEQ_LEN = 64  # tune: number of time-steps per example

def corpus_to_examples(corpus, seq_len=SEQ_LEN, token_to_int=token_to_int):
    X, y = [], []
    for seq in corpus:
        ints = [token_to_int[ev] for ev in seq if ev in token_to_int]
        if len(ints) <= seq_len:
            continue
        for i in range(len(ints) - seq_len):
            X.append(ints[i:i+seq_len])
            y.append(ints[i+seq_len])
    return np.array(X, dtype=np.int32), np.array(y, dtype=np.int32)

X, y = corpus_to_examples(corpus)
print("Examples:", X.shape, y.shape)

# Shuffle and optionally limit dataset size for quick experiments
perm = np.random.permutation(len(X))
X = X[perm]
y = y[perm]

MAX_SAMPLES = 40000
if len(X) > MAX_SAMPLES:
    X = X[:MAX_SAMPLES]
    y = y[:MAX_SAMPLES]
    print("Trimmed to", len(X), "samples")

"""# Step 9: Build the LSTM model (Embedding + LSTM)"""

EMBED_DIM = 128
LSTM_UNITS = 256

model = models.Sequential([
    layers.Input(shape=(SEQ_LEN,)),
    layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM),
    layers.LSTM(LSTM_UNITS, return_sequences=True),
    layers.Dropout(0.2),
    layers.LSTM(LSTM_UNITS // 2),
    layers.Dropout(0.2),
    layers.Dense(VOCAB_SIZE, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""# Step 10: Training with checkpoints, early stopping, and plots"""

BATCH_SIZE = 64
EPOCHS = 30

checkpoint_cb = callbacks.ModelCheckpoint("best_music_model.h5", save_best_only=True, monitor='loss')
early_cb = callbacks.EarlyStopping(patience=6, restore_best_weights=True, monitor='loss')

history = model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[checkpoint_cb, early_cb], validation_split=0.05)

# Plot training curves
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history.get('val_loss', []), label='val_loss')
plt.legend(); plt.title("Loss")
plt.subplot(1,2,2)
plt.plot(history.history.get('accuracy', []), label='train_acc')
plt.plot(history.history.get('val_accuracy', []), label='val_acc')
plt.legend(); plt.title("Accuracy")
plt.show()

"""# Step 11: Utility: temperature sampling and generation"""

def sample_with_temperature(preds, temperature=1.0):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds + 1e-9) / max(1e-9, temperature)
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

def generate_sequence(model, seed_seq_ints, length=256, temperature=1.0):
    seq = list(seed_seq_ints)
    for _ in range(length):
        x = np.array(seq[-SEQ_LEN:])[None, :]
        preds = model.predict(x, verbose=0)[0]
        idx = sample_with_temperature(preds, temperature)
        seq.append(int(idx))
    return seq

"""# Step 12: Convenience: pick seed and generate"""

# Choose a random seed or pick index
if len(X) == 0:
    raise ValueError("No training examples. Ensure MIDI files were parsed and examples created.")
seed_choice = random.randint(0, len(X)-1)  # pick random seed from training examples
seed = X[seed_choice]  # ints
print("Using random seed index:", seed_choice)

GEN_LENGTH = 512     # number of new tokens to generate
TEMPERATURE = 1.0    # try 0.6, 0.8, 1.0, 1.2
generated_ints = generate_sequence(model, seed, length=GEN_LENGTH, temperature=TEMPERATURE)
print("Generated sequence length (tokens):", len(generated_ints))

"""# Step 13: Convert token ints → MIDI"""

def ints_to_midi(int_sequence, int_to_token, time_step=TIME_STEP, outfile="generated.mid", program=0):
    pm = pretty_midi.PrettyMIDI()
    instrument = pretty_midi.Instrument(program=program)

    current_notes = dict()  # pitch -> start_time
    t = 0.0
    for idx in int_sequence:
        ev = int_to_token.get(int(idx))
        if ev is None:
            ev = ()
        active = set(ev)
        # end notes no longer active
        to_remove = []
        for p in list(current_notes.keys()):
            if p not in active:
                start_t = current_notes[p]
                end_t = t
                note_obj = pretty_midi.Note(velocity=100, pitch=int(p), start=start_t, end=end_t)
                instrument.notes.append(note_obj)
                to_remove.append(p)
        for p in to_remove:
            del current_notes[p]
        # start newly active notes
        for p in active:
            if p not in current_notes:
                current_notes[p] = t
        t += time_step

    # close remaining
    for p, start_t in current_notes.items():
        note_obj = pretty_midi.Note(velocity=100, pitch=int(p), start=start_t, end=t)
        instrument.notes.append(note_obj)

    pm.instruments.append(instrument)
    pm.write(outfile)
    return outfile

midi_out = "generated.mid"
# optionally drop the seed portion for a fresh piece: use generated_ints[SEQ_LEN:]
midi_out = ints_to_midi(generated_ints[SEQ_LEN:], int_to_token, time_step=TIME_STEP, outfile=midi_out)
print("Saved MIDI to", midi_out)

"""# Step 14: Render MIDI → WAV/MP3 (timidity + ffmpeg) and inline playback"""

!apt-get update -qq
!apt-get install -y timidity -qq

import subprocess
wav_out = "generated.wav"
mp3_out = "generated.mp3"

# Render to WAV with timidity
cmd = ["timidity", midi_out, "-Ow", "-o", wav_out]
print("Running:", " ".join(cmd))
subprocess.run(cmd, check=True)

# Convert WAV -> MP3 (ffmpeg already present on Colab)
cmd2 = ["ffmpeg", "-y", "-i", wav_out, mp3_out]
print("Running:", " ".join(cmd2))
subprocess.run(cmd2, check=True)

from IPython.display import Audio, display
print("Playing MP3 inline:")
display(Audio(mp3_out))
print("Files saved:", midi_out, wav_out, mp3_out)

"""# Step 15: Download links"""

from google.colab import files
print("You can download generated files now:")
files.download(midi_out)
files.download(mp3_out)

"""# Step 16: Extra diagnostics & reproducibility info"""

# Save package versions for reproducibility
with open("environment_versions.txt", "w") as f:
    import pkg_resources, sys
    f.write("python: " + sys.version + "\n")
    for pkg in ["tensorflow", "pretty_midi", "music21", "numpy"]:
        try:
            import importlib
            v = importlib.import_module(pkg).__version__
        except Exception:
            v = "?"
        f.write(f"{pkg}: {v}\n")
print("Saved environment_versions.txt")

print("Saved artifacts in current directory:")
for fname in ["best_music_model.h5", "token_to_int.json", "int_to_token.json", "generated.mid", "generated.wav", "generated.mp3", "environment_versions.txt"]:
    if os.path.exists(fname):
        print("-", fname)

"""# This project fulfills all requirements of Task 3: Music Generation with AI, including MIDI data collection, preprocessing using music21, LSTM-based training, music generation, and audio rendering"""